# AlphaTune-MambaSR

**Baseline reproduction for MambaLiteSR with Knowledge Distillation**

Reproducing baseline results: **Set5: 28.28 dB PSNR** using a 370K parameter student model with knowledge distillation.

## ğŸ¯ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Prepare Data
```bash
python scripts/download_datasets.py
```
This downloads benchmark datasets (Set5, Set14, BSD100, Urban100) and sets up the data structure.

### 3. Train Complete Pipeline
```bash
python train_pipeline.py
```
This runs the complete baseline reproduction:
1. **Teacher training** (large model, no KD)
2. **Student training** (with KD from teacher)  
3. **Benchmark evaluation** (all 4 datasets)

## ğŸ“Š Expected Results

| Dataset  | Baseline PSNR | Target SSIM |
|----------|---------------|-------------|
| Set5     | 28.28 dB      | 0.837       |
| Set14    | 25.39 dB      | 0.727       |
| BSD100   | 25.20 dB      | 0.693       |
| Urban100 | 22.57 dB      | 0.686       |

## ğŸ—‚ï¸ Project Structure

```
ğŸ“ AlphaTune-MambaSR/
â”œâ”€â”€ ğŸ¯ Training Scripts
â”‚   â”œâ”€â”€ train_teacher.py      # Step 1: Train large teacher model
â”‚   â”œâ”€â”€ train_student.py      # Step 2: Train student with KD
â”‚   â”œâ”€â”€ train_pipeline.py     # Complete automated pipeline
â”‚   â””â”€â”€ eval_benchmarks.py    # Step 3: Evaluate all benchmarks
â”œâ”€â”€ ğŸ”§ Core Implementation  
â”‚   â””â”€â”€ src/mambalitesr/
â”‚       â”œâ”€â”€ model.py          # MambaLiteSR architecture
â”‚       â”œâ”€â”€ blocks.py         # Mamba blocks & components
â”‚       â”œâ”€â”€ losses.py         # Distillation & perceptual losses
â”‚       â”œâ”€â”€ data.py           # Datasets & data loading
â”‚       â”œâ”€â”€ utils.py          # Metrics & utilities
â”‚       â”œâ”€â”€ config.py         # Hyperparameters
â”‚       â””â”€â”€ experiment_manager.py  # Versioned experiment tracking
â”œâ”€â”€ ğŸ“Š Outputs
â”‚   â””â”€â”€ runs/                 # Timestamped experiment results
â”‚       â”œâ”€â”€ latest/           # Symlink to latest experiment
â”‚       â”œâ”€â”€ mambalitesr_teacher_*/    # Teacher experiments
â”‚       â”œâ”€â”€ mambalitesr_student_*/    # Student experiments  
â”‚       â””â”€â”€ mambalitesr_evaluation_*/ # Evaluation results
â””â”€â”€ ğŸ“ Data & Docs
    â”œâ”€â”€ data/                 # Datasets (auto-downloaded)
    â”œâ”€â”€ scripts/              # Dataset download utilities
    â””â”€â”€ docs/                 # Paper & documentation
```

## âš™ï¸ Manual Training

If you prefer step-by-step control:

```bash
# Step 1: Train teacher model (large, no KD)
python train_teacher.py

# Step 2: Train student with knowledge distillation  
python train_student.py

# Step 3: Evaluate on all benchmarks
python eval_benchmarks.py
```

## ğŸ“Š Experiment Tracking

All experiments are automatically versioned with:
- âœ… **Timestamped directories** - No overwriting
- âœ… **Complete metrics** - PSNR, SSIM, training curves
- âœ… **Configuration backup** - All hyperparameters saved
- âœ… **Code snapshots** - Source code backup
- âœ… **Baseline comparisons** - Automatic performance gaps

**View latest results:**
```bash
python -c "
import json
from pathlib import Path
metrics_file = Path('runs/latest/metrics.json')
if metrics_file.exists():
    with open(metrics_file) as f:
        metrics = json.load(f)
    if 'evaluation' in metrics and 'summary' in metrics['evaluation']:
        avg_psnr = metrics['evaluation']['summary']['average_psnr']
        print(f'Latest Average PSNR: {avg_psnr:.2f} dB')
    else:
        print('Run eval_benchmarks.py to see results')
else:
    print('No experiments found. Run train_pipeline.py first.')
"
```

## ğŸ”§ Model Architecture

**Student Model (396K params):**
- Embed dim: 40, Blocks: 6, Low-rank: 6
- Scale: 4x, Mixers per block: 2

**Teacher Model (larger):**  
- Embed dim: 128, Blocks: 12, Low-rank: 16
- Used for knowledge distillation only

## ğŸ“ˆ Hyperparameters

Key settings in `src/mambalitesr/config.py`:
- **Knowledge Distillation:** `alpha=0.2` (best from baseline)
- **Training:** 1000 epochs, batch size 32, LR 1e-4
- **Optimization:** Learning rate warmup, early stopping
- **Data:** Random crops, augmentation (flips, rotations)

## ğŸš€ Advanced Usage

**Tune hyperparameters:**
Edit `src/mambalitesr/config.py` and rerun training.

**Enable perceptual loss:**
```python
beta = 0.1  # VGG19 perceptual loss
```

**Different KD weights:**
```python
alpha = 0.4  # Try 0.2, 0.4, 0.6 from baseline paper
```

## ğŸ“„ License

Apache-2.0 for this implementation. See original Mamba license if using `mamba_ssm`.
